{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57213a6e",
   "metadata": {},
   "source": [
    "# Scraping vacancies from habr.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7520a32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse page num. 1\n",
      "parse page num. 2\n",
      "parse page num. 3\n",
      "parse page num. 4\n",
      "parse page num. 5\n",
      "parse page num. 6\n",
      "parse page num. 7\n",
      "parse page num. 8\n",
      "parse page num. 9\n",
      "parse page num. 10\n",
      "parse page num. 11\n",
      "parse page num. 12\n",
      "parse page num. 13\n",
      "parse page num. 14\n",
      "parse page num. 15\n",
      "parse page num. 16\n",
      "parse page num. 17\n",
      "parse page num. 18\n",
      "parse page num. 19\n",
      "parse page num. 20\n",
      "parse page num. 21\n",
      "parse page num. 22\n",
      "parse page num. 23\n",
      "parse page num. 24\n",
      "Parse process is complete.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "check = None\n",
    "i = 1\n",
    "company = []\n",
    "position = []\n",
    "location = []\n",
    "salary = []\n",
    "skills = []\n",
    "    \n",
    "def extract(page):\n",
    "    vacancies = page.find_all(class_=\"vacancy-card__info\")\n",
    "    # Парсим страницу\n",
    "    for vacancy in vacancies:\n",
    "        company.append (vacancy.find(class_=\"link-comp link-comp--appearance-dark\").text) # компания\n",
    "        position.append (vacancy.find(class_=\"vacancy-card__title\").text) # Вакансия\n",
    "        location.append (vacancy.find(class_=\"vacancy-card__meta\").text) # Город, тип занятости\n",
    "        salary.append (vacancy.find(class_=\"vacancy-card__salary\").text) # зарплата\n",
    "        skills.append (vacancy.find(class_=\"vacancy-card__skills\").text) # скиллы\n",
    "   \n",
    "while check == None:    \n",
    "    url = \"https://career.habr.com/vacancies?page=\" + str(i) + \"&type=all&with_salary=true\"\n",
    "    site = requests.get(url)\n",
    "    soup = BeautifulSoup(site.text, 'html.parser')\n",
    "    check = soup.find(class_=\"no-content__title\")\n",
    "    if check == None:\n",
    "        extract(soup)\n",
    "        print(\"parse page num. \" +  str(i))\n",
    "        i += 1\n",
    "        time.sleep(4)\n",
    "    else:\n",
    "        time.sleep(4)\n",
    "        print ('Parse process is complete.')\n",
    "        break         \n",
    "\n",
    "        # Make DataFrame and save data in Excel\n",
    "dat = pd.DataFrame(list(zip(company, position, location, salary, skills)), columns =['Company', 'Position', 'Location', 'Salary', 'Skills'])\n",
    "dat.to_excel('data.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51ef23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
